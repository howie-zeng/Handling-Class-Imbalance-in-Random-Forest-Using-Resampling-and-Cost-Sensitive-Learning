results[[method]] <- list(
Metrics = metrics,
BestParams = best_params,
FinalModel = final_model,
Runtime = bayes_opt_time["elapsed"]
)
}
return(results)
}
run_pipeline_all <- function(df_list, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The outer results list will store results for each dataset
all_results <- list()
# Iterate over each dataset in the list
for (dataset_name in names(df_list)) {
cat("Processing dataset:", dataset_name, "\n")
data <- df_list[[dataset_name]]  # Extract the dataset
# Call the `run_pipeline` function for this dataset
dataset_results <- run_pipeline(data, target_col, sampling_methods)
# Store the results for this dataset
all_results[[dataset_name]] <- dataset_results
}
return(all_results)
}
results <- run_pipeline_all(data_model, target_col = "target")
results
stopCluster(cl)
explore_datasets(df_list)
stopCluster(cl)
registerDoSEQ()
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
library(package_name, character.only = TRUE)
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo)
} else if (install) {
detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
df_list <- process_all_dataframes(copy(data))
explore_datasets(df_list)
calculate_metrics <- function(true_labels, predicted_labels, predicted_probs, weights = NULL) {
# Input validation
if (length(true_labels) != length(predicted_labels)) {
stop("`true_labels` and `predicted_labels` must have the same length.")
}
# Confusion Matrix
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(true_labels))
# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
# Weighted Accuracy (if weights provided)
if (!is.null(weights)) {
weighted_accuracy <- sum((true_labels == predicted_labels) * weights) / sum(weights)
} else {
weighted_accuracy <- accuracy
}
# Precision, Recall, F1 Score
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)
# ROC AUC
if (!is.null(predicted_probs)) {
roc_obj <- roc(true_labels, predicted_probs, levels = c(0, 1), direction = "<")
auc_value <- auc(roc_obj)
} else {
auc_value <- NA
}
# Output all metrics as a list
list(
ConfusionMatrix = conf_matrix,
Accuracy = accuracy,
WeightedAccuracy = weighted_accuracy,
Precision = precision,
Recall = recall,
F1Score = f1_score,
ROCAUC = auc_value
)
}
df_split_results <- split_data(copy(df_list), 'target')
data_model = copy(df_split_results)
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df$train)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
num_cores <- parallel::detectCores() - 3  # Reserve one core for system use
cl <- parallel::makeCluster(num_cores)
registerDoParallel(cl)
clusterExport(cl,c('df_split_results'))
clusterEvalQ(cl,expr= {
library(ranger)
library(pROC)
})
run_pipeline <- function(data, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The results list will store the outcome for each sampling method
results <- list()
for (method in sampling_methods) {
cat("Sampling method:", method, "\n")
# Apply the specified sampling method to the training set
sampled_data <- apply_sampling(data$train, target_col, method)
train <- sampled_data
test <- data$validation
train[[target_col]] <- factor(train[[target_col]])
test[[target_col]] <- factor(test[[target_col]])
# Define the bounds for Bayesian Optimization
# Adjust these as needed based on domain knowledge
bounds <- list(
min_node_size = c(1, 10),
num_trees = c(10, 200)
)
# Define the objective function to be optimized
# This function will be called internally by bayesOpt
objective_function <- function(mtry, min_node_size, num_trees) {
# Train a Random Forest model using ranger
rf_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = round(sqrt(ncol(train))),
num.trees = round(num_trees),
min.node.size = round(min_node_size),
probability = TRUE,  # Enable probability predictions
num.threads = parallel::detectCores() - 1
)
# Predict probabilities on the test set
predictions <- predict(rf_model, test)  # 'response' for classification
predicted_probs <- predictions$predictions[, 2]          # Extract probabilities for the positive class
# Compute ROC AUC
roc_auc <- pROC::auc(pROC::roc(test$target, predicted_probs, levels = c(0, 1), direction = "<"))
# Return the ROC AUC as the objective score
return(list(Score = as.numeric(roc_auc)))
}
# Run Bayesian Optimization
bayes_opt_time <- system.time({
opt_result <- bayesOpt(
FUN = objective_function,
bounds = bounds,
initPoints = 7,  # Specify either `initPoints`
iters.n = num_cores,    # Number of optimization iterations
iters.k = num_cores,
verbose = 1,
parallel = TRUE
)
})
# Extract the best hyperparameters found
best_params <- getBestPars(opt_result)
# Train the final model using the best parameters
final_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = sqrt(ncol(train)),
num.trees = round(best_params[["num_trees"]]),
min.node.size = round(best_params[["min_node_size"]]),
probability = TRUE,                     # Enable probability predictions
num.threads = parallel::detectCores() - 1  # Use available cores for parallelization
)
# Evaluate the final model on the test set
final_probs <- predict(final_model, test, type = "response")$predictions[, 2]
final_labels <- ifelse(final_probs >= 0.5, 1, 0)
metrics <- calculate_metrics(test$target, final_labels, final_probs)
# Store the results
results[[method]] <- list(
Metrics = metrics,
BestParams = best_params,
FinalModel = final_model,
Runtime = bayes_opt_time["elapsed"]
)
}
return(results)
}
run_pipeline_all <- function(df_list, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The outer results list will store results for each dataset
all_results <- list()
# Iterate over each dataset in the list
for (dataset_name in names(df_list)) {
cat("Processing dataset:", dataset_name, "\n")
data <- df_list[[dataset_name]]  # Extract the dataset
# Call the `run_pipeline` function for this dataset
dataset_results <- run_pipeline(data, target_col, sampling_methods)
# Store the results for this dataset
all_results[[dataset_name]] <- dataset_results
}
return(all_results)
}
results <- run_pipeline_all(data_model, target_col = "target")
stopCluster(cl)
registerDoSEQ()
stopCluster(cl)
print(cl)
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
library(package_name, character.only = TRUE)
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo)
} else if (install) {
detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
library(package_name, character.only = TRUE)
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo)
} else if (install) {
detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
df_list <- process_all_dataframes(copy(data))
explore_datasets(df_list)
calculate_metrics <- function(true_labels, predicted_labels, predicted_probs, weights = NULL) {
# Input validation
if (length(true_labels) != length(predicted_labels)) {
stop("`true_labels` and `predicted_labels` must have the same length.")
}
# Confusion Matrix
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(true_labels))
# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
# Weighted Accuracy (if weights provided)
if (!is.null(weights)) {
weighted_accuracy <- sum((true_labels == predicted_labels) * weights) / sum(weights)
} else {
weighted_accuracy <- accuracy
}
# Precision, Recall, F1 Score
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)
# ROC AUC
if (!is.null(predicted_probs)) {
roc_obj <- roc(true_labels, predicted_probs, levels = c(0, 1), direction = "<")
auc_value <- auc(roc_obj)
} else {
auc_value <- NA
}
# Output all metrics as a list
list(
ConfusionMatrix = conf_matrix,
Accuracy = accuracy,
WeightedAccuracy = weighted_accuracy,
Precision = precision,
Recall = recall,
F1Score = f1_score,
ROCAUC = auc_value
)
}
df_split_results <- split_data(copy(df_list), 'target')
data_model = copy(df_split_results)
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df$train)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
num_cores <- parallel::detectCores() - 3  # Reserve one core for system use
cl <- parallel::makeCluster(num_cores)
registerDoParallel(cl)
clusterExport(cl,c('df_split_results'))
clusterEvalQ(cl,expr= {
library(ranger)
library(pROC)
})
run_pipeline <- function(data, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The results list will store the outcome for each sampling method
results <- list()
for (method in sampling_methods) {
cat("Sampling method:", method, "\n")
# Apply the specified sampling method to the training set
sampled_data <- apply_sampling(data$train, target_col, method)
train <- sampled_data
test <- data$validation
train[[target_col]] <- factor(train[[target_col]])
test[[target_col]] <- factor(test[[target_col]])
# Define the bounds for Bayesian Optimization
# Adjust these as needed based on domain knowledge
bounds <- list(
min_node_size = c(1, 10),
num_trees = c(10, 200)
)
# Define the objective function to be optimized
# This function will be called internally by bayesOpt
objective_function <- function(mtry, min_node_size, num_trees) {
# Train a Random Forest model using ranger
rf_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = round(sqrt(ncol(train))),
num.trees = round(num_trees),
min.node.size = round(min_node_size),
probability = TRUE,  # Enable probability predictions
num.threads = parallel::detectCores() - 1
)
# Predict probabilities on the test set
predictions <- predict(rf_model, test)  # 'response' for classification
predicted_probs <- predictions$predictions[, 2]          # Extract probabilities for the positive class
# Compute ROC AUC
roc_auc <- pROC::auc(pROC::roc(test$target, predicted_probs, levels = c(0, 1), direction = "<"))
# Return the ROC AUC as the objective score
return(list(Score = as.numeric(roc_auc)))
}
# Run Bayesian Optimization
bayes_opt_time <- system.time({
opt_result <- bayesOpt(
FUN = objective_function,
bounds = bounds,
initPoints = 7,  # Specify either `initPoints`
iters.n = num_cores,    # Number of optimization iterations
iters.k = num_cores,
verbose = 1,
parallel = TRUE
)
})
# Extract the best hyperparameters found
best_params <- getBestPars(opt_result)
# Train the final model using the best parameters
final_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = sqrt(ncol(train)),
num.trees = round(best_params[["num_trees"]]),
min.node.size = round(best_params[["min_node_size"]]),
probability = TRUE,                     # Enable probability predictions
num.threads = parallel::detectCores() - 1  # Use available cores for parallelization
)
# Evaluate the final model on the test set
final_probs <- predict(final_model, test, type = "response")$predictions[, 2]
final_labels <- ifelse(final_probs >= 0.5, 1, 0)
metrics <- calculate_metrics(test$target, final_labels, final_probs)
# Store the results
results[[method]] <- list(
Metrics = metrics,
BestParams = best_params,
FinalModel = final_model,
Runtime = bayes_opt_time["elapsed"]
)
}
return(results)
}
run_pipeline_all <- function(df_list, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The outer results list will store results for each dataset
all_results <- list()
# Iterate over each dataset in the list
for (dataset_name in names(df_list)) {
cat("Processing dataset:", dataset_name, "\n")
data <- df_list[[dataset_name]]  # Extract the dataset
# Call the `run_pipeline` function for this dataset
dataset_results <- run_pipeline(data, target_col, sampling_methods)
# Store the results for this dataset
all_results[[dataset_name]] <- dataset_results
}
return(all_results)
}
results <- run_pipeline_all(data_model, target_col = "target")
results
df_modata_model
# Function to run the custom algorithms pipeline
run_algorithms_pipeline <- function(data, target_col, methods = c("SMOTEBagging", "SMOTEBoost", "BalanceCascade", "cost-sensitive-ranger")) {
results <- list()  # Store results for each method
# Iterate over methods
for (method in methods) {
cat("Running method:", method, "\n")
if (method == "SMOTEBagging") {
# BBaging Method
result <- run_bbaging(data, target_col)
results[["BBaging"]] <- result
} else if (method == "SMOTEBoost") {
# BBoost Method
result <- run_bboost(data, target_col)
results[["BBoost"]] <- result
} else if (method == "BalanceCascade") {
# Balance Cascade Method
result <- run_balance_cascade(data, target_col)
results[["BalanceCascade"]] <- result
} else if (method == "cost-sensitive-ranger") {
# Cost-Sensitive Ranger
result <- run_cost_sensitive_ranger(data, target_col)
results[["CostSensitiveRanger"]] <- result
}
}
return(results)
}
# Function to run BBaging
run_bbaging <- function(data, target_col) {
x <- data$train[, !colnames(data$train) %in% target_col]
y <- data$train[[target_col]]
model <- bbaging.data.frame(x, y, numBag = 40, type = "SMOTEBagging", allowParallel = TRUE)
predictions <- predict(model, data$validation[, !colnames(data$validation) %in% target_col])
roc_auc <- pROC::auc(pROC::roc(data$validation[[target_col]], as.numeric(predictions)))
return(list(Model = model, ROC_AUC = roc_auc))
}
# Function to run BBoost
run_bboost <- function(data, target_col) {
x <- data$train[, !colnames(data$train) %in% target_col]
y <- data$train[[target_col]]
model <- bboost.data.frame(x, y, iter = 40, type = "SMOTEBoost")
predictions <- predict(model, data$validation[, !colnames(data$validation) %in% target_col], type = "class")
roc_auc <- pROC::auc(pROC::roc(data$validation[[target_col]], as.numeric(predictions)))
return(list(Model = model, ROC_AUC = roc_auc))
}
# Function to run BalanceCascade
run_balance_cascade <- function(data, target_col) {
x <- data$train[, !colnames(data$train) %in% target_col]
y <- data$train[[target_col]]
model <- BalanceCascade.data.frame(x, y, iter = 4)
predictions <- predict(model, data$validation[, !colnames(data$validation) %in% target_col], type = "class")
roc_auc <- pROC::auc(pROC::roc(data$validation[[target_col]], as.numeric(predictions)))
return(list(Model = model, ROC_AUC = roc_auc))
}
# Function to run cost-sensitive ranger
run_cost_sensitive_ranger <- function(data, target_col) {
train <- data$train
train[[target_col]] <- factor(train[[target_col]])
weights <- ifelse(train[[target_col]] == levels(train[[target_col]])[1], 0.1, 1)
model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
case.weights = weights,
probability = TRUE,
num.trees = 200
)
predictions <- predict(model, data$validation, type = "response")$predictions[, 2]
roc_auc <- pROC::auc(pROC::roc(data$validation[[target_col]], predictions))
return(list(Model = model, ROC_AUC = roc_auc))
}
# Iterate over datasets
results_algorithms <- list()
for (dataset_name in names(data_model$`HR Dataset`)) {
cat("Processing dataset:", dataset_name, "\n")
data <- data_model[[dataset_name]]
# Run the custom pipeline
dataset_results <- run_custom_pipeline(data, target_col = "target")
results_algorithms[[dataset_name]] <- dataset_results
}

metrics_df <- rbind(
metrics_df,
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Accuracy",
Value = accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Weighted Accuracy",
Value = weighted_accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Precision",
Value = precision
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Recall",
Value = recall
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "F1",
Value = f1
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "AUC",
Value = auc
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Specificity",
Value = specificity
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "G-Mean",
Value = gmean
)
)
}
# for (metric_name in names(metrics_data)) {
#   metrics_df <- rbind(
#     metrics_df,
#     data.frame(
#       Method = method_name,
#       Dataset = data_name,
#       Model = model_name,
#       Metric = metric_name,
#       Value = metrics_data[[metric_name]]
#     )
#   )
# }
}
}
}
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create a list to store plots for each metric
plot_list <- list()
# # Generate a plot for each metric
unique_metrics <- unique(metrics_df$Metric)
# custom_palette <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))(length(unique_models))
for (metric_name in unique_metrics) {
# Filter data for the current metric
metric_subset <- subset(metrics_df, Metric == metric_name)
# Plot performance of models for the current metric across methods
p <- ggplot(metric_subset, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
# Use position dodge for clear bar separation
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
# Add a line to separate groups for clarity
geom_hline(yintercept = 0, color = "black") +
# Improve readability with better fonts and clean axis labels
labs(
title = paste("Performance Comparison for Metric:", metric_name),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
# Enhance visual aesthetics
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16), # Centered and bold title
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),    # Slight rotation for clarity
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),  # Remove vertical grid lines
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
# Add better color contrast for models
scale_fill_brewer(palette = "Paired") +
# Scale the y-axis dynamically for better data visibility
scale_y_continuous(labels = scales::percent_format(scale = 1))  # Display as percentages if needed
# Save the plot in the list
plot_list[[metric_name]] <- p
ggsave(filename = paste0("images/performance/Metric_", metric_name, ".png"), plot = p, width = 8, height = 6)
print(p)
}
method_ranking_1 <- c(
"None",
"Cost-Sensitive Learning",
"SMOTE",
"ADASYN",
"Tomek Links",
"SMOTETomek",
"SMOTEBagging",
"SMOTEBoosting",
"BalanceCascade",
"EasyEnsemble"
)
metrics_scaled <- metrics_df %>%
group_by(Dataset, Metric) %>%
mutate(Value = rescale(Value, to = c(0, 1))) %>%
ungroup()
metrics_scaled <- metrics_scaled %>%
group_by(Metric, Model) %>%
summarise(Value = mean(Value), .groups = "drop")
metrics_scaled$Model <- factor(metrics_scaled$Model, levels = method_ranking_1)
# Split data by 'Metric'
metrics_split <- metrics_scaled %>% group_split(Metric)
# Generate plots for each group
for (df in metrics_split) {
metric_name <- df$Metric[1]
p <- ggplot(df, aes(x = Model, y = Value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(
title = paste("Normalized Metric:", metric_name),
x = "Model",
y = "Metric Value",
fill = "Method"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
scale_fill_brewer(palette = "Paired") +
scale_y_continuous(labels = scales::percent_format(scale = 1))
ggsave(filename = paste0("images/nomrliazed_performance/Metric_", metric_name, ".png"), plot = p, width = 8, height = 6)
print(p)  # Display each plot
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
metrics_df <- metrics_df %>%
group_by(Dataset) %>%
mutate(Normalized_Value = (Value - min(Value)) / (max(Value) - min(Value))) %>%
ungroup()
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Normalized_Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
TN
TP
FP
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
library(ebmc)
source('model.R')
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo, force=TRUE)
} else if (install) {
# detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo, force=TRUE)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning", install=TRUE)
data <- load_data()
data <- load_data('data')
data <- load_data("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/MyPackage")
data <- load_data("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/MyPackage/data")
df_list <- process_all_dataframes(copy(data))
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
library(ImbalanceRF)
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr"))
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
library(ImbalanceRF)
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
x <- PimaIndiansDiabetes %>% select(-diabetes)
y <- as.factor(ifelse(PimaIndiansDiabetes$diabetes == "pos", 1, 0))
table(y)
# Train SMOTEBagging model
model <- bbaging(x, y, numBag = 10, type = "SMOTEBagging")
# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
# Train SMOTEBoost model
model <- bboost(x, y, iter = 20, type = "SMOTEBoost")
SMOTE
# Train EasyEnsemble model
model <- EasyEnsemble(x, y, iter = 4)
# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
# Train BalanceCascade model
model <- BalanceCascade(x, y, iter = 4)
# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
# plot distribution of y
table(y)
# Apply SMOTETomek to the dataset
balanced_data <- SMOTETomek(x, y, percOver = 100)
# plot distribution of y
table(balanced_data$y)
# Install required packages
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))
# Load libraries
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)
# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
library(ImbalanceRF)
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution
# Train SMOTEBagging model
model <- bbaging(x, y, numBag = 10, type = "SMOTEBagging")
# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)
# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
# Train SMOTEBoost model
model <- bboost(x, y, iter = 20, type = "SMOTEBoost")
bboost
bboost?
s
?bboost
??bboost
devtools::build()
detach("package:ImbalanceRF", unload = TRUE)
# Install required packages
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))
# Load libraries
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)
# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
# Prepare features and labels
x <- pima %>% select(-diabetes)
# Install required packages
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))
# Load libraries
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)
# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
library(ImbalanceRF)
# Install required packages
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))
# Load libraries
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes
# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution
# Train SMOTEBagging model
model <- bbaging(x, y, numBag = 10, type = "SMOTEBagging")
# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)
# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
# Train SMOTEBoost model
model <- bboost(x, y, iter = 20, type = "SMOTEBoost")

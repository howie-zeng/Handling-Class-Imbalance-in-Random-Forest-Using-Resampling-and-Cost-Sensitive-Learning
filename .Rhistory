"Insurance Dataset" = df_insurance,
"Credit Card Default Dataset" = df_credit_card_default,
"Credit Card Approval Dataset" = df_credit_card_approval
)
# Rename target columns
df_list <- lapply(names(df_list), function(name) {
df <- df_list[[name]]
target_col <- switch(name,
"Credit Card Fraud Dataset" = "Class",
"Diabetes Dataset" = "Diabetes_binary",
"Bank Fraud Dataset" = "fraud_bool",
"Insurance Dataset" = "Response",
"Credit Card Default Dataset" = "default.payment.next.month",
"Credit Card Approval Dataset" = "target",
NULL
)
if (!is.null(target_col)) {
df <- rename_target_column(df, target_col)
}
return(df)
})
names(df_list) <- c("HR Dataset", "Bank Fraud Dataset", "Credit Card Fraud Dataset",
"Diabetes Dataset", "Insurance Dataset", "Credit Card Default Dataset",
"Credit Card Approval Dataset")
return(df_list)
}
rename_target_column <- function(data, original_name, new_name = "target") {
if (original_name %in% colnames(data)) {
colnames(data)[colnames(data) == original_name] <- new_name
data[[new_name]] <- as.factor(data[[new_name]])
}
return(data)
}
process_credit_card_data <- function(df_credit_data, df_credit_card_approval) {
# Check required columns
required_cols_data <- c("STATUS", "ID")
required_cols_approval <- c("CODE_GENDER", "FLAG_OWN_CAR", "FLAG_OWN_REALTY",
"FLAG_WORK_PHONE", "FLAG_PHONE", "FLAG_EMAIL", "DAYS_BIRTH",
"AMT_INCOME_TOTAL", "NAME_INCOME_TYPE", "NAME_HOUSING_TYPE",
"NAME_EDUCATION_TYPE", "NAME_FAMILY_STATUS")
if (!all(required_cols_data %in% colnames(df_credit_data))) {
stop("Missing required columns in credit data")
}
if (!all(required_cols_approval %in% colnames(df_credit_card_approval))) {
stop("Missing required columns in credit card approval data")
}
# Update dep_value based on STATUS
df_credit_data <- df_credit_data %>%
mutate(dep_value = ifelse(STATUS %in% c("2", "3", "4", "5"), "Yes", "No"))
# Group by ID and determine risk status
overdue_status <- df_credit_data %>%
group_by(ID) %>%
summarize(risk = ifelse(any(dep_value == "Yes"), 1, 0), .groups = "drop")
# Merge with application data and create target
df_credit_card_approval <- df_credit_card_approval %>%
inner_join(overdue_status, by = "ID") %>%
mutate(target = factor(risk))
# Binary features
df_credit_card_approval <- df_credit_card_approval %>%
mutate(
Gender = as.factor(ifelse(CODE_GENDER == "M", 1, 0)),
Car = as.factor(ifelse(FLAG_OWN_CAR == "Y", 1, 0)),
Reality = as.factor(ifelse(FLAG_OWN_REALTY == "Y", 1, 0)),
wkphone = as.factor(FLAG_WORK_PHONE),
phone = as.factor(FLAG_PHONE),
email = as.factor(FLAG_EMAIL)
)
# Age
df_credit_card_approval <- df_credit_card_approval %>%
mutate(Age = -DAYS_BIRTH %/% 365) %>%
mutate(gp_Age = cut(
Age,
breaks = quantile(Age, probs = seq(0, 1, 0.2), na.rm = TRUE),
labels = c("lowest", "low", "medium", "high", "highest"),
include.lowest = TRUE
))
# Income groups
df_credit_card_approval <- df_credit_card_approval %>%
mutate(
inc = AMT_INCOME_TOTAL / 10000,
gp_inc = cut(
inc,
breaks = quantile(inc, probs = seq(0, 1, 0.3), na.rm = TRUE),
labels = c("low", "medium", "high"),
include.lowest = TRUE
)
)
# Categorical features
df_credit_card_approval <- df_credit_card_approval %>%
mutate(
inctp = recode(NAME_INCOME_TYPE, "Pensioner" = "State servant", "Student" = "State servant"),
houtp = as.factor(NAME_HOUSING_TYPE),
edutp = as.factor(ifelse(NAME_EDUCATION_TYPE == "Academic degree", "Higher education", NAME_EDUCATION_TYPE)),
famtp = as.factor(NAME_FAMILY_STATUS)
)
return(df_credit_card_approval)
}
df_list <- load_data()
df_list
calculate_percentage <- function(data, target_col) {
if (!(target_col %in% names(data))) {
stop(paste("Column", target_col, "not found in the dataset."))
}
data %>%
mutate(!!sym(target_col) := as.character(!!sym(target_col))) %>%
group_by(!!sym(target_col)) %>%
summarize(Count = n()) %>%
mutate(Percentage = (Count / sum(Count)) * 100)
}
explore_datasets <- function(data_list) {
combined_results <- data.frame()  # Empty data frame to store results
for (name in names(data_list)) {
cat("Dataset:", name, "\n")
# Check if the target column exists
if ("target" %in% colnames(data_list[[name]])) {
target_values <- unique(data_list[[name]]$target)
cat("Unique target values:", paste(target_values, collapse = ", "), "\n")
# Calculate percentage table
result_table <- calculate_percentage(data_list[[name]], "target")
result_table <- result_table %>% mutate(Dataset = name)  # Add dataset name
# Combine into the final results table
combined_results <- bind_rows(combined_results, result_table)
} else {
cat("Target column not found.\n")
}
cat("\n")  # Line break for readability
}
return(combined_results)
}
# Explore datasets and get the combined result
final_table <- explore_datasets(df_list)
# Display the combined table
print(final_table)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
Warning message:
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
library(package_name, character.only = TRUE)
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo)
} else if (install) {
detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
df_list <- process_all_dataframes(copy(data))
explore_datasets(df_list)
df_split_results <- split_data(copy(df_list), 'target')
sum(is.na((df_split_results$`Bank Fraud Dataset`$train)))
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
library(package_name, character.only = TRUE)
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo)
} else if (install) {
detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
df_list <- process_all_dataframes(copy(data))
explore_datasets(df_list)
df_split_results <- split_data(copy(df_list), 'target')
sum(is.na(df_split_results$`Bank Fraud Dataset`))
sum(is.na(df_split_results$`Bank Fraud Dataset`$train))
sum(is.na(df_split_results$`Bank Fraud Dataset`$validation))
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
data_model = copy(df_split_results)
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
check_na_in_list <- function(df_list) {
sapply(df_list$train, function(df) any(is.na(df)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
check_na_in_list <- function(df_list) {
sapply(df_list, function(df$train) any(is.na(df)))
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
check_na_in_list <- function(df_list) {
sapply(df_list, function(df$train) any(is.na(df)))
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df$train)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
sum(is.na(data_model$`Bank Fraud Dataset`$train))
sum(is.na(data_model$`Credit Card Fraud Dataset`$train))
sum(is.na(data_model$`Credit Card Fraud Dataset`$train))
sum(is.na(data_model$`Diabetes Dataset`$train))
sum(is.na(data_model$`Insurance Dataset`$train))
sum(is.na(data_model$`Credit Card Default Dataset`$train))
sum(is.na(data_model$`Credit Card Default Dataset`$train))
df_split_results <- split_data(copy(df_list), 'target')
data_model = copy(df_split_results)
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df$train)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(data_model)
na_check_results
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
num_cores <- parallel::detectCores() - 1  # Reserve one core for system use
cl <- parallel::makeCluster(num_cores)
registerDoParallel(cl)
clusterExport(cl,c('df_split_results'))
clusterEvalQ(cl,expr= {
library(ranger)
library(pROC)
})
run_pipeline <- function(data, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The results list will store the outcome for each sampling method
results <- list()
for (method in sampling_methods) {
cat("Sampling method:", method, "\n")
# Apply the specified sampling method to the training set
sampled_data <- apply_sampling(data$train, target_col, method)
train <- sampled_data
test <- data$validation
train[[target_col]] <- factor(train[[target_col]])
test[[target_col]] <- factor(test[[target_col]])
# Define the bounds for Bayesian Optimization
# Adjust these as needed based on domain knowledge
bounds <- list(
min_node_size = c(1, 10),
num_trees = c(10, 200)
)
# Define the objective function to be optimized
# This function will be called internally by bayesOpt
objective_function <- function(mtry, min_node_size, num_trees) {
# Train a Random Forest model using ranger
rf_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = round(sqrt(ncol(train))),
num.trees = round(num_trees),
min.node.size = round(min_node_size),
probability = TRUE,  # Enable probability predictions
num.threads = parallel::detectCores() - 1
)
# Predict probabilities on the test set
predictions <- predict(rf_model, test)  # 'response' for classification
predicted_probs <- predictions$predictions[, 2]          # Extract probabilities for the positive class
# Compute ROC AUC
roc_auc <- pROC::auc(pROC::roc(test$target, predicted_probs, levels = c(0, 1), direction = "<"))
# Return the ROC AUC as the objective score
return(list(Score = as.numeric(roc_auc)))
}
# Run Bayesian Optimization
bayes_opt_time <- system.time({
opt_result <- bayesOpt(
FUN = objective_function,
bounds = bounds,
initPoints = 7,  # Specify either `initPoints`
iters.n = num_cores,    # Number of optimization iterations
iters.k = num_cores,
verbose = 1,
parallel = TRUE
)
})
# Extract the best hyperparameters found
best_params <- getBestPars(opt_result)
# Train the final model using the best parameters
final_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = sqrt(ncol(train)),
num.trees = round(best_params[["num_trees"]]),
min.node.size = round(best_params[["min_node_size"]]),
probability = TRUE,                     # Enable probability predictions
num.threads = parallel::detectCores() - 1  # Use available cores for parallelization
)
# Evaluate the final model on the test set
final_probs <- predict(final_model, test, type = "response")$predictions[, 2]
final_labels <- ifelse(final_probs >= 0.5, 1, 0)
metrics <- calculate_metrics(test$target, final_labels, final_probs)
# Store the results
results[[method]] <- list(
Metrics = metrics,
BestParams = best_params,
FinalModel = final_model,
Runtime = bayes_opt_time["elapsed"]
)
}
return(results)
}
run_pipeline_all <- function(df_list, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The outer results list will store results for each dataset
all_results <- list()
# Iterate over each dataset in the list
for (dataset_name in names(df_list)) {
cat("Processing dataset:", dataset_name, "\n")
data <- df_list[[dataset_name]]  # Extract the dataset
# Call the `run_pipeline` function for this dataset
dataset_results <- run_pipeline(data, target_col, sampling_methods)
# Store the results for this dataset
all_results[[dataset_name]] <- dataset_results
}
return(all_results)
}
results <- run_pipeline_all(data_model, target_col = "target")
calculate_metrics <- function(true_labels, predicted_labels, predicted_probs, weights = NULL) {
# Input validation
if (length(true_labels) != length(predicted_labels)) {
stop("`true_labels` and `predicted_labels` must have the same length.")
}
# Confusion Matrix
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(true_labels))
# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
# Weighted Accuracy (if weights provided)
if (!is.null(weights)) {
weighted_accuracy <- sum((true_labels == predicted_labels) * weights) / sum(weights)
} else {
weighted_accuracy <- accuracy
}
# Precision, Recall, F1 Score
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)
# ROC AUC
if (!is.null(predicted_probs)) {
roc_obj <- roc(true_labels, predicted_probs, levels = c(0, 1), direction = "<")
auc_value <- auc(roc_obj)
} else {
auc_value <- NA
}
# Output all metrics as a list
list(
ConfusionMatrix = conf_matrix,
Accuracy = accuracy,
WeightedAccuracy = weighted_accuracy,
Precision = precision,
Recall = recall,
F1Score = f1_score,
ROCAUC = auc_value
)
}
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
num_cores <- parallel::detectCores() - 1  # Reserve one core for system use
cl <- parallel::makeCluster(num_cores)
registerDoParallel(cl)
clusterExport(cl,c('df_split_results'))
clusterEvalQ(cl,expr= {
library(ranger)
library(pROC)
})
run_pipeline <- function(data, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The results list will store the outcome for each sampling method
results <- list()
for (method in sampling_methods) {
cat("Sampling method:", method, "\n")
# Apply the specified sampling method to the training set
sampled_data <- apply_sampling(data$train, target_col, method)
train <- sampled_data
test <- data$validation
train[[target_col]] <- factor(train[[target_col]])
test[[target_col]] <- factor(test[[target_col]])
# Define the bounds for Bayesian Optimization
# Adjust these as needed based on domain knowledge
bounds <- list(
min_node_size = c(1, 10),
num_trees = c(10, 200)
)
# Define the objective function to be optimized
# This function will be called internally by bayesOpt
objective_function <- function(mtry, min_node_size, num_trees) {
# Train a Random Forest model using ranger
rf_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = round(sqrt(ncol(train))),
num.trees = round(num_trees),
min.node.size = round(min_node_size),
probability = TRUE,  # Enable probability predictions
num.threads = parallel::detectCores() - 1
)
# Predict probabilities on the test set
predictions <- predict(rf_model, test)  # 'response' for classification
predicted_probs <- predictions$predictions[, 2]          # Extract probabilities for the positive class
# Compute ROC AUC
roc_auc <- pROC::auc(pROC::roc(test$target, predicted_probs, levels = c(0, 1), direction = "<"))
# Return the ROC AUC as the objective score
return(list(Score = as.numeric(roc_auc)))
}
# Run Bayesian Optimization
bayes_opt_time <- system.time({
opt_result <- bayesOpt(
FUN = objective_function,
bounds = bounds,
initPoints = 7,  # Specify either `initPoints`
iters.n = num_cores,    # Number of optimization iterations
iters.k = num_cores,
verbose = 1,
parallel = TRUE
)
})
# Extract the best hyperparameters found
best_params <- getBestPars(opt_result)
# Train the final model using the best parameters
final_model <- ranger::ranger(
formula = as.formula(paste(target_col, "~ .")),
data = train,
mtry = sqrt(ncol(train)),
num.trees = round(best_params[["num_trees"]]),
min.node.size = round(best_params[["min_node_size"]]),
probability = TRUE,                     # Enable probability predictions
num.threads = parallel::detectCores() - 1  # Use available cores for parallelization
)
# Evaluate the final model on the test set
final_probs <- predict(final_model, test, type = "response")$predictions[, 2]
final_labels <- ifelse(final_probs >= 0.5, 1, 0)
metrics <- calculate_metrics(test$target, final_labels, final_probs)
# Store the results
results[[method]] <- list(
Metrics = metrics,
BestParams = best_params,
FinalModel = final_model,
Runtime = bayes_opt_time["elapsed"]
)
}
return(results)
}
run_pipeline_all <- function(df_list, target_col, sampling_methods = c("none", "smote", "tomek", "adasyn", "smotemek")) { #"none", "smote", "tomek", "adasyn", "smotemek"
# The outer results list will store results for each dataset
all_results <- list()
# Iterate over each dataset in the list
for (dataset_name in names(df_list)) {
cat("Processing dataset:", dataset_name, "\n")
data <- df_list[[dataset_name]]  # Extract the dataset
# Call the `run_pipeline` function for this dataset
dataset_results <- run_pipeline(data, target_col, sampling_methods)
# Store the results for this dataset
all_results[[dataset_name]] <- dataset_results
}
return(all_results)
}
results <- run_pipeline_all(data_model, target_col = "target")
results
stopCluster(cl)
explore_datasets(df_list)
stopCluster(cl)
registerDoSEQ()

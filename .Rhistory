Model = model_name,
Metric = "Accuracy",
Value = accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Weighted Accuracy",
Value = weighted_accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Precision",
Value = precision
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Recall",
Value = recall
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "F1",
Value = f1
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "AUC",
Value = auc
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Specificity",
Value = specificity
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "G-Mean",
Value = gmean
)
)
}
# for (metric_name in names(metrics_data)) {
#   metrics_df <- rbind(
#     metrics_df,
#     data.frame(
#       Method = method_name,
#       Dataset = data_name,
#       Model = model_name,
#       Metric = metric_name,
#       Value = metrics_data[[metric_name]]
#     )
#   )
# }
}
}
}
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
# custom_palette <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))(length(unique_models))
for (metric_name in unique_metrics) {
# Filter data for the current metric
metric_subset <- subset(metrics_df, Metric == metric_name)
# Plot performance of models for the current metric across methods
p <- ggplot(metric_subset, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
# Use position dodge for clear bar separation
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
# Add a line to separate groups for clarity
geom_hline(yintercept = 0, color = "black") +
# Improve readability with better fonts and clean axis labels
labs(
title = paste("Performance Comparison for Metric:", metric_name),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
# Enhance visual aesthetics
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16), # Centered and bold title
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),    # Slight rotation for clarity
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),  # Remove vertical grid lines
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
# Add better color contrast for models
scale_fill_brewer(palette = "Paired") +
# Scale the y-axis dynamically for better data visibility
scale_y_continuous(labels = scales::percent_format(scale = 1))  # Display as percentages if needed
# Save the plot in the list
plot_list[[metric_name]] <- p
ggsave(filename = paste0("images/performance/Metric_", metric_name, ".png"), plot = p, width = 12, height = 9)
print(p)
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
metrics_df <- metrics_df %>%
group_by(Dataset) %>%
mutate(Normalized_Value = (Value - min(Value)) / (max(Value) - min(Value))) %>%
ungroup()
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Normalized_Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(scales)
library(ggplot2)
library(tidyr)
library(tidyverse)
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
metrics_df <- metrics_df %>%
group_by(Dataset) %>%
mutate(Normalized_Value = (Value - min(Value)) / (max(Value) - min(Value))) %>%
ungroup()
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Normalized_Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
df_list <- process_all_dataframes(copy(data))
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
library(ebmc)
source('model.R')
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo, force=TRUE)
} else if (install) {
# detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo, force=TRUE)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
library(ebmc)
source('model.R')
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo, force=TRUE)
} else if (install) {
# detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo, force=TRUE)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
library(ParBayesianOptimization)
library(ranger)
library(pROC)
library(doParallel)
library(ebmc)
source('model.R')
seed <- 2024
set.seed(seed)
target_col = 'target'
# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
if (!requireNamespace(package_name, quietly = TRUE)) {
cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
devtools::install_github(github_repo, force=TRUE)
} else if (install) {
# detach("package:ImbalanceRF", unload = TRUE)
devtools::install_github(github_repo, force=TRUE)
} else {
cat(paste("Package", package_name, "already installed. Loading it...\n"))
}
# Load the package
library(package_name, character.only = TRUE)
}
# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)
data <- load_data()
df_list <- process_all_dataframes(copy(data))
check_na_in_list <- function(df_list) {
sapply(df_list, function(df) any(is.na(df$train)))
}
# Apply the function to df_split_results
na_check_results <- check_na_in_list(df_list)
na_check_results
df_list$`HR Dataset`
ncol(df_list$`HR Dataset`)
ncol(df_list$`Credit Card Fraud Dataset`)
ncol(df_list$`Diabetes Dataset`)
ncol(df_list$`Insurance Dataset`)
ncol(df_list$`Credit Card Default Dataset`)
ncol(df_list$`Credit Card Approval Dataset`)
final_results_algorithmics <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/final_results_algorithmics.rds")
final_results_cost_sensitive_learning <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/final_results_cost_sensitive_learning.rds")
final_results_resampling <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/results_results_resampling.rds")
# df_temp <- final_results_algorithmics
# for (data_name in names(df_temp)) {
#   for (model_name in names(df_temp[[data_name]])) {
#     print(paste("Data Name:", data_name, "| Model Name:", model_name))
#   }
# }
for (data_name in names(final_results_cost_sensitive_learning)){
names(final_results_cost_sensitive_learning[[data_name]]) <- 'cost-sensitive-learning'
}
final_results_algorithmics$`HR Dataset`$SMOTEBagging$Metrics$ROCAUC
final_results_algorithmics$`HR Dataset`$SMOTEBagging$Metrics$ConfusionMatrix

# recall <- confusion_matrix$byClass[["Recall"]]
# f1 <- confusion_matrix$byClass[["F1"]]
# auc <- metrics_data$ROCAUC
# specificity <- confusion_matrix$byClass[["Specificity"]]
# gmean <- sqrt(recall * specificity)
metrics_df <- rbind(
metrics_df,
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Accuracy",
Value = accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Weighted Accuracy",
Value = weighted_accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Precision",
Value = precision
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Recall",
Value = recall
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "F1",
Value = f1
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "AUC",
Value = auc
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Specificity",
Value = specificity
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "G-Mean",
Value = gmean
)
)
}
# for (metric_name in names(metrics_data)) {
#   metrics_df <- rbind(
#     metrics_df,
#     data.frame(
#       Method = method_name,
#       Dataset = data_name,
#       Model = model_name,
#       Metric = metric_name,
#       Value = metrics_data[[metric_name]]
#     )
#   )
# }
}
}
}
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create a list to store plots for each metric
plot_list <- list()
# # Generate a plot for each metric
unique_metrics <- unique(metrics_df$Metric)
# custom_palette <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))(length(unique_models))
for (metric_name in unique_metrics) {
# Filter data for the current metric
metric_subset <- subset(metrics_df, Metric == metric_name)
# Plot performance of models for the current metric across methods
p <- ggplot(metric_subset, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
# Use position dodge for clear bar separation
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
# Add a line to separate groups for clarity
geom_hline(yintercept = 0, color = "black") +
# Improve readability with better fonts and clean axis labels
labs(
title = paste("Performance Comparison for Metric:", metric_name),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
# Enhance visual aesthetics
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16), # Centered and bold title
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),    # Slight rotation for clarity
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),  # Remove vertical grid lines
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
# Add better color contrast for models
scale_fill_brewer(palette = "Paired") +
# Scale the y-axis dynamically for better data visibility
scale_y_continuous(labels = scales::percent_format(scale = 1))  # Display as percentages if needed
# Save the plot in the list
plot_list[[metric_name]] <- p
ggsave(filename = paste0("images/performance/Metric_", metric_name, ".png"), plot = p, width = 8, height = 6)
print(p)
}
recall
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(scales)
library(ggplot2)
library(tidyr)
library(tidyverse)
final_results_algorithmics <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/final_results_algorithmics.rds")
final_results_cost_sensitive_learning <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/final_results_cost_sensitive_learning.rds")
final_results_resampling <- readRDS("E:/OneDrive/Cornell/Fall 2024/STSCI 6520/Final Project/results_results_resampling.rds")
# df_temp <- final_results_algorithmics
# for (data_name in names(df_temp)) {
#   for (model_name in names(df_temp[[data_name]])) {
#     print(paste("Data Name:", data_name, "| Model Name:", model_name))
#   }
# }
for (data_name in names(final_results_cost_sensitive_learning)){
names(final_results_cost_sensitive_learning[[data_name]]) <- 'cost-sensitive-learning'
}
# wrong calculation for recall
method_ranking <- c("none", "cost-sensitive-learning", "smote", "adasyn", "tomek", "smotemek", "SMOTEBagging", "SMOTEBoosting", "BalanceCascade", "EasyEnsemble")
rename_datasets <- c(
"Credit Card Fraud Dataset" = "Credit Card Fraud (0.17%)",
"Diabetes Dataset" = "Diabetes (13.9%)",
"Insurance Dataset" = "Customer Capture (16.0%)",
"Credit Card Approval Dataset" = "Credit Card Approval (1.6%)",
"Credit Card Default Dataset" = "Loan Default (22.1%)",
"HR Dataset" = "% of leaving a job (24.9%)"
)
dataset_order <- c(
"Credit Card Fraud (0.17%)",
"Diabetes (13.9%)",
"Customer Capture (16.0%)",
"Credit Card Approval (1.6%)",
"Loan Default (22.1%)",
"% of leaving a job (24.9%)"
)
rename_models <- c(
"cost-sensitive-learning" = "Cost-Sensitive Learning",
"smote" = "SMOTE",
"adasyn" = "ADASYN",
"tomek" = "Tomek Links",
"smotemek" = "SMOTETomek",
"SMOTEBagging" = "SMOTEBagging",
"SMOTEBoosting" = "SMOTEBoosting",
"BalanceCascade" = "BalanceCascade",
"EasyEnsemble" = "EasyEnsemble",
"none" = "None"
)
result_sets <- list(
"Algorithmic" = final_results_algorithmics,
"CostSensitiveLearning" = final_results_cost_sensitive_learning,
"Resampling" = final_results_resampling
)
metrics_df <- data.frame(Dataset = character(),
Model = character(),
Metric = character(),
Value = numeric(),
stringsAsFactors = FALSE)
for (method_name in names(result_sets)) {
df_temp <- result_sets[[method_name]]
for (data_name in names(df_temp)) {
for (model_name in names(df_temp[[data_name]])) {
metrics_data <- df_temp[[data_name]][[model_name]][["Metrics"]]
# metrics_data <- metrics_data[names(metrics_data) %in% c("ConfusionMatrix")]
if ("ConfusionMatrix" %in% names(metrics_data)) {
# get accuracy, weighted accuracy, precision, recall, f1, auc
confusion_matrix <- metrics_data[["ConfusionMatrix"]]
TP <- confusion_matrix$table[2, 2]
FP <- confusion_matrix$table[2, 1]
TN <- confusion_matrix$table[1, 1]
FN <- confusion_matrix$table[1, 2]
accuracy <- (TP + TN) / sum(confusion_matrix$table)
weighted_accuracy <- (TP / (TP + FN) + TN / (TN + FP)) / 2
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
auc <- metrics_data$ROCAUC
specificity <- TN / (TN + FP)
gmean <- sqrt(recall * specificity)
# accuracy <- confusion_matrix$overall[["Accuracy"]]
# weighted_accuracy <- confusion_matrix$byClass[["Balanced Accuracy"]]
# precision <- confusion_matrix$byClass[["Precision"]]
# recall <- confusion_matrix$byClass[["Recall"]]
# f1 <- confusion_matrix$byClass[["F1"]]
# auc <- metrics_data$ROCAUC
# specificity <- confusion_matrix$byClass[["Specificity"]]
# gmean <- sqrt(recall * specificity)
metrics_df <- rbind(
metrics_df,
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Accuracy",
Value = accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Weighted Accuracy",
Value = weighted_accuracy
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Precision",
Value = precision
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Recall",
Value = recall
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "F1",
Value = f1
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "AUC",
Value = auc
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "Specificity",
Value = specificity
),
data.frame(
Method = method_name,
Dataset = data_name,
Model = model_name,
Metric = "G-Mean",
Value = gmean
)
)
}
# for (metric_name in names(metrics_data)) {
#   metrics_df <- rbind(
#     metrics_df,
#     data.frame(
#       Method = method_name,
#       Dataset = data_name,
#       Model = model_name,
#       Metric = metric_name,
#       Value = metrics_data[[metric_name]]
#     )
#   )
# }
}
}
}
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create a list to store plots for each metric
plot_list <- list()
# # Generate a plot for each metric
unique_metrics <- unique(metrics_df$Metric)
# custom_palette <- colorRampPalette(RColorBrewer::brewer.pal(8, "Dark2"))(length(unique_models))
for (metric_name in unique_metrics) {
# Filter data for the current metric
metric_subset <- subset(metrics_df, Metric == metric_name)
# Plot performance of models for the current metric across methods
p <- ggplot(metric_subset, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
# Use position dodge for clear bar separation
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
# Add a line to separate groups for clarity
geom_hline(yintercept = 0, color = "black") +
# Improve readability with better fonts and clean axis labels
labs(
title = paste("Performance Comparison for Metric:", metric_name),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
# Enhance visual aesthetics
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16), # Centered and bold title
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),    # Slight rotation for clarity
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),  # Remove vertical grid lines
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
# Add better color contrast for models
scale_fill_brewer(palette = "Paired") +
# Scale the y-axis dynamically for better data visibility
scale_y_continuous(labels = scales::percent_format(scale = 1))  # Display as percentages if needed
# Save the plot in the list
plot_list[[metric_name]] <- p
ggsave(filename = paste0("images/performance/Metric_", metric_name, ".png"), plot = p, width = 8, height = 6)
print(p)
}
method_ranking_1 <- c(
"None",
"Cost-Sensitive Learning",
"SMOTE",
"ADASYN",
"Tomek Links",
"SMOTETomek",
"SMOTEBagging",
"SMOTEBoosting",
"BalanceCascade",
"EasyEnsemble"
)
metrics_scaled <- metrics_df %>%
group_by(Dataset, Metric) %>%
mutate(Value = rescale(Value, to = c(0, 1))) %>%
ungroup()
metrics_scaled <- metrics_scaled %>%
group_by(Metric, Model) %>%
summarise(Value = mean(Value), .groups = "drop")
metrics_scaled$Model <- factor(metrics_scaled$Model, levels = method_ranking_1)
# Split data by 'Metric'
metrics_split <- metrics_scaled %>% group_split(Metric)
# Generate plots for each group
for (df in metrics_split) {
metric_name <- df$Metric[1]
p <- ggplot(df, aes(x = Model, y = Value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(
title = paste("Normalized Metric:", metric_name),
x = "Model",
y = "Metric Value",
fill = "Method"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank()
) +
scale_fill_brewer(palette = "Paired") +
scale_y_continuous(labels = scales::percent_format(scale = 1))
ggsave(filename = paste0("images/nomrliazed_performance/Metric_", metric_name, ".png"), plot = p, width = 8, height = 6)
print(p)  # Display each plot
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
metrics_list <- list()
# Loop through result sets and extract metrics
for (method in names(result_sets)) {
method_results <- result_sets[[method]]
for (dataset in names(method_results)) {
dataset_results <- method_results[[dataset]]
for (model in names(dataset_results)) {
runtime_value <- dataset_results[[model]][["Runtime"]]
# Append the results as a list element
metrics_list <- append(metrics_list, list(
data.frame(
Method = method,
Dataset = dataset,
Model = model,
Metric = "Runtime",
Value = runtime_value,
stringsAsFactors = FALSE
)
))
}
}
}
# Combine all collected rows into a single data frame
metrics_df <- do.call(rbind, metrics_list)
metrics_df <- metrics_df %>%
group_by(Dataset) %>%
mutate(Normalized_Value = (Value - min(Value)) / (max(Value) - min(Value))) %>%
ungroup()
# Add rankings and rename models and datasets
metrics_df$Rank <- match(metrics_df$Model, method_ranking)
metrics_df$Model <- rename_models[metrics_df$Model]
metrics_df$Dataset <- rename_datasets[metrics_df$Dataset]
# Create and save plots for each metric
library(ggplot2)
plot_list <- list()
unique_metrics <- unique(metrics_df$Metric)
for (metric in unique_metrics) {
# Subset data for the current metric
metric_data <- subset(metrics_df, Metric == metric)
# Generate the plot
p <- ggplot(metric_data, aes(x = Dataset, y = Normalized_Value, fill = reorder(Model, Rank))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.7), width = 0.6) +
geom_hline(yintercept = 0, color = "black") +
labs(
title = paste("Performance Comparison for Metric:", metric),
x = "Dataset",
y = "Metric Value",
fill = "Model"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
axis.text.x = element_text(angle = 30, hjust = 1, size = 11),
axis.text.y = element_text(size = 11),
legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 11),
panel.grid.major.x = element_blank(),
panel.grid.minor = element_blank()
) +
scale_fill_brewer(palette = "Paired")
# Dynamically save and display the plot
plot_list[[metric]] <- p
ggsave(filename = paste0("images/runtime/Metric_", metric, ".png"), plot = p, width = 8, height = 6)
print(p)
}
TN
TP
FP

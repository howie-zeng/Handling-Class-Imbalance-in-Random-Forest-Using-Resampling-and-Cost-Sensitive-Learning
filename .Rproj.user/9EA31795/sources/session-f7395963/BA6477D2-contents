---
title: "Final Project"
author: "Howard"
date: "2024-12-08"
output: html_document
---


•	Datasets
o	Credit Card Fraud Detection
	https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
o	Default of Credit Card Clients Dataset
	https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset
<!-- o	Predict Droughts using Weather & Soil Data
	https://www.kaggle.com/datasets/cdminix/us-drought-meteorological-data -->
o	HR Analytics: Job Change of Data Scientists
	https://www.kaggle.com/datasets/arashnic/hr-analytics-job-change-of-data-scientists 
o	Diabetes Health Indicators Dataset
	https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv
o	Bank Account Fraud Dataset Suite
	https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022 

Interest in insurance detection
https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice


Reference
https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf
https://link.springer.com/article/10.1023/A:1010933404324
https://ieeexplore.ieee.org/document/5128907
https://ieeexplore.ieee.org/abstract/document/8122151

devtools::document()
devtools::check()
devtools::clean_dll()
devtools::build()


```{r setup, include=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(randomForest)
library(missForest)
library(caret)
library(robustbase)
library(smotefamily)
library(parallel)
library(data.table)
seed <- 2024
set.seed(seed)

# Function to check if the package exists and install if missing
install_and_load <- function(package_name, github_repo, install=FALSE) {
  if (!requireNamespace(package_name, quietly = TRUE)) {
    cat(paste("Package", package_name, "not found. Installing from GitHub...\n"))
    devtools::install_github(github_repo)
  } else if (install) {
    detach("package:ImbalanceRF", unload = TRUE)
    devtools::install_github(github_repo)
  } else {
    cat(paste("Package", package_name, "already installed. Loading it...\n"))
  }
  
  # Load the package
  library(package_name, character.only = TRUE)
}

# Example Usage
install_and_load("ImbalanceRF", "howie-zeng/MyPackage", install=TRUE)

data <- load_data()



# devtools::document()
# devtools::check()
# devtools::clean_dll()
# devtools::build()

```

```{r}
df_list <- process_all_dataframes(copy(data))
explore_datasets(df_list)


df_list$'Insurance Dataset'
```

```{r}
# # Function to improve plotting
# plot_categorical_distributions <- function(data, dataset_name, max_levels = 10, ncol = 2) {
#   # Identify categorical features
#   categorical_features <- names(data)[sapply(data, function(col) is.character(col) || is.factor(col))]
# 
#   # Prepare data for plotting
#   categorical_data <- data %>%
#     select(all_of(categorical_features)) %>%
#     pivot_longer(cols = everything(), names_to = "Feature", values_to = "Value") %>%
#     group_by(Feature, Value) %>%
#     summarize(Count = n(), .groups = "drop") %>%
#     arrange(Feature, desc(Count)) %>%
#     group_by(Feature) %>%
#     slice_max(order_by = Count, n = max_levels)  # Limit to top levels for each feature
# 
#   # Create the plot
#   ggplot(categorical_data, aes(x = reorder(Value, Count), y = Count, fill = Feature)) +
#     geom_bar(stat = "identity") +
#     facet_wrap(~Feature, scales = "free", ncol = ncol) +
#     labs(title = paste("Categorical Feature Distributions in", dataset_name),
#          x = "Value",
#          y = "Count") +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels
#           legend.position = "none") +
#     scale_fill_brewer(palette = "Set2")
# }
# 
# 
split_train <- function(data, target_col, train_ratio = 0.75, maxiter = 2, ntree = 20, scale_cols = NULL, seed = 2024) {
  # Set seed for reproducibility
  set.seed(seed)

  # Step 1: Impute missing values
  imputed_data <- missForest(xmis = data, maxiter = maxiter, ntree = ntree)$ximp

  # Step 2: Convert the target column to factor if not already
  imputed_data[[target_col]] <- as.factor(imputed_data[[target_col]])

  # Step 3: Automatically detect numerical columns to scale if scale_cols is NULL
  if (is.null(scale_cols)) {
    scale_cols <- names(imputed_data)[sapply(imputed_data, is.numeric) & names(imputed_data) != target_col]
  }

  # Step 4: Apply Robust Scaling to specified columns
  robust_scaler <- function(x) {
    (x - median(x, na.rm = TRUE)) / IQR(x, na.rm = TRUE)
  }
  imputed_data[scale_cols] <- lapply(imputed_data[scale_cols], robust_scaler)

  # Step 5: Stratified split using caret's createDataPartition
  trainIndex <- createDataPartition(imputed_data[[target_col]], p = train_ratio, list = FALSE)

  # Step 6: Split into training and validation sets
  train_data <- imputed_data[trainIndex, ]
  validation_data <- imputed_data[-trainIndex, ]

  # Return both datasets
  list(train = train_data, validation = validation_data)
}


# plot_categorical_distributions(df_hr, "HR Dataset")

# for (name in names(df_list)) {
#   plot_all_distributions(df_list[[name]], name)
# }

split_data(df_list, 'target')

```




# Preprocessing 

## HR Dataset

```{r}
df_hr <-
  df_hr %>% rename(relevant_experience = relevent_experience) %>%
  mutate(
    relevant_experience = str_replace(
      relevant_experience,
      pattern = "relevent",
      replacement = "relevant"
    )
  ) %>%
  mutate_if(is.character, as.factor) %>% mutate(target = as.factor(target)) %>% mutate(city_development_index = as.factor(city_development_index))

df_hr <- df_hr %>%
  select(-enrollee_id, -city, -gender) %>%
  mutate(city_development_index = as.numeric(city_development_index))
```

```{r}
split_result <- split_train(data = df_hr, target_col = "target", train_ratio = 0.75)

train_data <- split_result$train
validation_data <- split_result$validation
```

```{r}
rf_model <- randomForest(
  target ~ .,
  data = train_data,
  ntree = 200,
  mtry = 3,
  importance = TRUE
)
print(rf_model)
varImpPlot(rf_model)
```
```{r}
predicted_validation <- predict(rf_model, validation_data)
confusionMatrix(predicted_validation, validation_data$target)

```

## Bank Fraud Dataset
```{r}
split_train_test_by_month <- function(data, target_col, month_col, train_months, test_months, seed = 2024) {
  # Set seed for reproducibility
  set.seed(seed)
  
  # Step 1: Split data by month
  train_data <- data %>% filter(!!sym(month_col) %in% train_months)
  test_data <- data %>% filter(!!sym(month_col) %in% test_months)
  
  # Step 2: Separate features and target
  X_train <- train_data %>% select(-all_of(target_col))
  y_train <- train_data[[target_col]]
  
  X_test <- test_data %>% select(-all_of(target_col))
  y_test <- test_data[[target_col]]
  
  # Step 3: Identify numeric columns for robust scaling
  num_cols <- names(X_train)[sapply(X_train, is.numeric)]
  
  robust_scaler <- function(x) {
    (x - median(x, na.rm = TRUE)) / IQR(x, na.rm = TRUE)
  }
  
  # Step 4: Apply robust scaling to numeric columns
  X_train[num_cols] <- lapply(X_train[num_cols], robust_scaler)
  X_test[num_cols] <- lapply(X_test[num_cols], robust_scaler)
  
  # Return scaled and split data
  list(
    X_train = X_train,
    y_train = y_train,
    X_test = X_test,
    y_test = y_test
  )
}

split_result <- split_train_test_by_month(
  data = df_bank_fraud,
  target_col = "target",
  month_col = "month",
  train_months = 0:5,
  test_months = 6:7
)

X_train <- split_result$X_train
y_train <- split_result$y_train
X_test <- split_result$X_test
y_test <- split_result$y_test


```




## Credit Card Fraud Dataset
```{r}

split_result <- split_train(data = df_credit_card_fraud, target_col = "target", train_ratio = 0.75)

```

## Credit Default
```{r}


colnames(df_credit_card_default) <- c("ID", "LIMIT_BAL", "SEX", "EDUCATION", "MARRIAGE", "AGE", 
                  "PAY_SEPT", "PAY_AUG", "PAY_JUL", "PAY_JUN", "PAY_MAY", "PAY_APR", 
                  "BILL_AMT_SEPT", "BILL_AMT_AUG", "BILL_AMT_JUL", "BILL_AMT_JUN", 
                  "BILL_AMT_MAY", "BILL_AMT_APR", "PAY_AMT_SEPT", "PAY_AMT_AUG", 
                  "PAY_AMT_JUL", "PAY_AMT_JUN", "PAY_AMT_MAY", "PAY_AMT_APR", "target")
df_credit_card_default <- df_credit_card_default %>%
  mutate(
    EDUCATION = ifelse(EDUCATION %in% c(0, 5, 6), 4, EDUCATION),
    MARRIAGE = ifelse(MARRIAGE == 0, 3, MARRIAGE)
  )

# Feature Engineering: Create new features
df_credit_card_default <- df_credit_card_default %>%
  mutate(
    Payment_Sum = PAY_SEPT + PAY_AUG + PAY_JUL + PAY_JUN + PAY_MAY + PAY_APR,
    Dues = (BILL_AMT_APR + BILL_AMT_MAY + BILL_AMT_JUN + BILL_AMT_JUL + BILL_AMT_SEPT) - 
            (PAY_AMT_APR + PAY_AMT_MAY + PAY_AMT_JUN + PAY_AMT_JUL + PAY_AMT_AUG + PAY_AMT_SEPT)
  )

# Convert categorical variables to factors
df_credit_card_default$SEX <- factor(df_credit_card_default$SEX, levels = c(1, 2), labels = c("MALE", "FEMALE"))
df_credit_card_default$EDUCATION <- factor(df_credit_card_default$EDUCATION, labels = c("Graduate School", "University", "High School", "Others"))
df_credit_card_default$MARRIAGE <- factor(df_credit_card_default$MARRIAGE, labels = c("Married", "Single", "Others"))
df_credit_card_default$target <- factor(df_credit_card_default$target)


split_train(data = df_credit_card_default, target_col = "target", train_ratio = 0.75)

```

## Diabetes
```{r}
# Specify the columns to drop
columns_to_drop <- c("Fruits", "Veggies", "Sex", "CholCheck", "AnyHealthcare")

# Drop the specified columns
df_diabetes <- df_diabetes %>%
  select(-all_of(columns_to_drop))
```

##Interest in insurance detection
https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice
```{r}
split_train(data = df_insurance, target_col = "target", train_ratio = 0.75)

```


## Credit Card Approval
```{r}

split_train(data = df_credit_card_approval, target_col = "target", train_ratio = 0.75)

```

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calculate_metrics.R
\name{calculate_metrics}
\alias{calculate_metrics}
\title{Calculate Performance Metrics}
\usage{
calculate_metrics(
  true_labels,
  predicted_labels,
  predicted_probs,
  weights = NULL
)
}
\arguments{
\item{true_labels}{A vector of true class labels.}

\item{predicted_labels}{A vector of predicted class labels.}

\item{predicted_probs}{A vector of predicted probabilities for the positive class (optional).}

\item{weights}{Optional weights for observations (default is NULL).}
}
\value{
A list containing:
\item{ConfusionMatrix}{The confusion matrix.}
\item{ROCAUC}{The area under the ROC curve (AUC).}
}
\description{
This function calculates performance metrics for a classification model, including the confusion matrix and ROC AUC.
}
\examples{
true_labels <- c(0, 1, 1, 0, 1)
predicted_labels <- c(0, 1, 0, 0, 1)
predicted_probs <- c(0.2, 0.8, 0.4, 0.1, 0.9)
calculate_metrics(true_labels, predicted_labels, predicted_probs)
}

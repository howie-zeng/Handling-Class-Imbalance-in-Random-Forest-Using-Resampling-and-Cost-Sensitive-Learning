---
title: "Handling Class Imbalance in Random Forest Using Resampling and Cost-Sensitive Learning"
author: "Haozhe (Howard) Zeng"
output: html_document
---

# Handling Class Imbalance in Random Forest Using Resampling and Cost-Sensitive Learning

Class imbalance is a common challenge in machine learning, particularly in classification tasks. This document demonstrates the use of **resampling techniques** (e.g., oversampling and undersampling) and **cost-sensitive learning methods** to address imbalanced data in random forest models.

### Key Topics Covered:
1. **Bagging-Based Methods**:
   - SMOTEBagging, RUSBagging, ROSBagging, Random Balance Bagging (RBBagging)
2. **Boosting-Based Methods**:
   - SMOTEBoost, RUSBoost, AdaBoost, Cost-Sensitive AdaBoost (AdaC2)
3. **Specialized Ensemble Methods**:
   - EasyEnsemble, BalanceCascade
4. **Hybrid Methods**:
   - SMOTETomek (SMOTE combined with Tomek link removal)

---

# Required Libraries

Before proceeding, ensure the necessary packages are installed and loaded:

```{r}
# Install required packages
install.packages(c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest"))

# Load libraries
library(rpart)
library(foreach)
library(doParallel)
library(RANN)
library(pROC)
library(caret)
library(mlbench)
library(dplyr)
library(randomForest)

# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning")
library(ImbalanceRF)


# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes


# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution

```

# Bagging-Based Methods (bbaging)
The `bbaging` function implements bagging-based resampling methods, including:

- Random Under-Sampling (RUSBagging)
- Random Over-Sampling (ROSBagging)
- SMOTE (Synthetic Minority Oversampling Technique) Bagging
- Random Balance Bagging (RBBagging)

Example: SMOTEBagging
```{r}
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes

# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution

# Train SMOTEBagging model
model <- bbaging(x, y, numBag = 10, type = "SMOTEBagging")

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)

```


# Boosting-Based Methods (bboost)
The `bboost` function applies boosting with resampling or cost-sensitive approaches, such as:

- AdaBoost
- SMOTEBoost
- RUSBoost
- Cost-Sensitive AdaBoost (AdaC2)

Example: SMOTEBoost

```{r}
# Train SMOTEBoost model
model <- bboost(x, y, iter = 20, type = "SMOTEBoost")

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)


```

# EasyEnsemble
EasyEnsemble creates multiple balanced datasets by undersampling the majority class and training individual classifiers.

Example: EasyEnsemble

```{r}
# Train EasyEnsemble model
model <- EasyEnsemble(x, y, iter = 4)

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)


```


# Balance Cascade
Balance Cascade iteratively trains classifiers while removing easy-to-classify majority instances.

Example: Balance Cascade

```{r}
# Train BalanceCascade model
model <- BalanceCascade(x, y, iter = 4)

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)


```

# Hybrid Methods: SMOTETomek
SMOTETomek combines SMOTE oversampling with Tomek link removal for better balancing of the dataset.

Example: SMOTETomek

```{r}
# Plot original class distribution
table(y)

# Apply SMOTETomek
balanced_data <- SMOTETomek(x, y, percOver = 100)

# Plot new class distribution
table(balanced_data$y)


```


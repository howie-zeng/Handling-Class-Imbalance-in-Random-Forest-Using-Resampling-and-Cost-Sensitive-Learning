---
title: "Handling Class Imbalance in Random Forest Using Resampling and Cost-Sensitive Learning"
author: "Haozhe (Howard) Zeng"
output:
  pdf_document: default
  html_document: default
---

# Handling Class Imbalance in Random Forest Using Resampling and Cost-Sensitive Learning

Class imbalance is a common challenge in machine learning, particularly in classification tasks. This document demonstrates the use of **resampling techniques** (e.g., oversampling and undersampling) and **cost-sensitive learning methods** to address imbalanced data in random forest models.

### Key Topics Covered:
1. **Bagging-Based Methods**:
   - SMOTEBagging, RUSBagging, ROSBagging, Random Balance Bagging (RBBagging)
2. **Boosting-Based Methods**:
   - SMOTEBoost, RUSBoost, AdaBoost, Cost-Sensitive AdaBoost (AdaC2)
3. **Specialized Ensemble Methods**:
   - EasyEnsemble, BalanceCascade
4. **Hybrid Methods**:
   - SMOTETomek (SMOTE combined with Tomek link removal)

---

# Required Libraries

Before proceeding, ensure the necessary packages are installed and loaded:

```{r message=TRUE, warning=TRUE, include=FALSE}
# Install and load ImbalanceRF package
devtools::install_github("howie-zeng/Handling-Class-Imbalance-in-Random-Forest-Using-Resampling-and-Cost-Sensitive-Learning", force = TRUE)
library(ImbalanceRF)
```

```{r message=TRUE, warning=TRUE, include=FALSE}
required_packages <- c("rpart", "foreach", "doParallel", "RANN", "pROC", "mlbench", "dplyr", "randomForest")

# Check and install missing packages
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

# Load libraries
lapply(required_packages, library, character.only = TRUE)
```

```{r}
# Load dataset
data("PimaIndiansDiabetes")
pima <- PimaIndiansDiabetes

# Prepare features and labels
x <- pima %>% select(-diabetes)
y <- as.factor(ifelse(pima$diabetes == "pos", 1, 0))
table(y) # Check class distribution
```

# Bagging-Based Methods (bbaging)
The `bbaging` function implements bagging-based resampling methods, including:

- Random Under-Sampling (RUSBagging)
- Random Over-Sampling (ROSBagging)
- SMOTE (Synthetic Minority Oversampling Technique) Bagging
- Random Balance Bagging (RBBagging)

### Parameters for `bbaging`:
- **`x`**: A data frame containing the predictor variables.
- **`y`**: A factor representing the response variable.
- **`numBag`**: The number of bagging iterations to perform. Default is `10`.
- **`type`**: The type of bagging method to use. Options include:
  - `"SMOTEBagging"`: Uses SMOTE for oversampling.
  - `"RUSBagging"`: Applies random undersampling.
  - `"ROSBagging"`: Performs random oversampling.
  - `"RBBagging"`: Uses random balance bagging.

Example: SMOTEBagging

```{r}
# Train SMOTEBagging model
model <- bbaging(x, y, numBag = 10, type = "SMOTEBagging")

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
```

# Boosting-Based Methods (bboost)
The `bboost` function applies boosting with resampling or cost-sensitive approaches, such as:

- AdaBoost
- SMOTEBoost
- RUSBoost
- Cost-Sensitive AdaBoost (AdaC2)

### Parameters for `bboost`:
- **`x`**: A data frame containing the predictor variables.
- **`y`**: A factor representing the response variable.
- **`iter`**: The number of boosting iterations. Default is `20`.
- **`type`**: The type of boosting method to use. Options include:
  - `"AdaBoost"`: Standard AdaBoost.
  - `"SMOTEBoost"`: Combines boosting with SMOTE.
  - `"RUSBoost"`: Combines boosting with random undersampling.
  - `"AdaC2"`: Cost-sensitive AdaBoost.

Example: SMOTEBoost

```{r}
# Train SMOTEBoost model
model <- bboost(x, y, iter = 20, type = "SMOTEBoost")

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
```

# EasyEnsemble
EasyEnsemble creates multiple balanced datasets by undersampling the majority class and training individual classifiers.

### Parameters for `EasyEnsemble`:
- **`x`**: A data frame containing the predictor variables.
- **`y`**: A factor representing the response variable.
- **`iter`**: The number of ensemble iterations. Default is `4`.
- **`allowParallel`**: A logical indicating whether to enable parallel computation. Default is `FALSE`.

Example: EasyEnsemble

```{r}
# Train EasyEnsemble model
model <- EasyEnsemble(x, y, iter = 4)

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
```

# Balance Cascade
Balance Cascade iteratively trains classifiers while removing easy-to-classify majority instances.

### Parameters for `BalanceCascade`:
- **`x`**: A data frame containing the predictor variables.
- **`y`**: A factor representing the response variable.
- **`iter`**: The number of cascade iterations. Default is `4`.

Example: Balance Cascade

```{r}
# Train BalanceCascade model
model <- BalanceCascade(x, y, iter = 4)

# Predictions
predictions_prob <- predict(model, x, type = "probability")[, 2]
predictions_label <- ifelse(predictions_prob > 0.5, 1, 0)

# Calculate metrics
metrics <- calculate_metrics(y, predictions_label, predictions_prob)
print(metrics)
```

# Hybrid Methods: SMOTETomek
SMOTETomek combines SMOTE oversampling with Tomek link removal for better balancing of the dataset.

### Parameters for `SMOTETomek`:
- **`x`**: A data frame containing the predictor variables.
- **`y`**: A factor representing the response variable.
- **`percOver`**: The percentage of oversampling to apply. Default is `100`.
- **`k`**: The number of nearest neighbors to use in SMOTE. Default is `5`.

Example: SMOTETomek

```{r}
# Plot original class distribution
print("Before")
table(y)

# Apply SMOTETomek
balanced_data <- SMOTETomek(x, y, percOver = 100)

# Plot new class distribution
print("After")
table(balanced_data$y)
```
